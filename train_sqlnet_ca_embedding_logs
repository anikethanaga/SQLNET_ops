Script started on Sunday 10 November 2019 01:15:14 AM IST
]0;team9@nvidia-DGX-Station: ~/17IT208/SQLNet[01;32mteam9@nvidia-DGX-Station[00m:[01;34m~/17IT208/SQLNet[00m$ cat typescriptls[Kpython train.py --ca --train_emb
Loading from original dataset
Loading data from data/train_tok.jsonl
Loading data from data/train_tok.tables.jsonl
Loading data from data/dev_tok.jsonl
Loading data from data/dev_tok.tables.jsonl
Loading data from data/test_tok.jsonl
Loading data from data/test_tok.tables.jsonl
Load used word embedding
Using trainable embedding
Using trainable embedding
Using trainable embedding
Using column attention on aggregator predicting
Using column attention on selection predicting
Using column attention on where predicting
Loading from saved_model/old_sqlnet_ca.agg_model
Loading from saved_model/old_sqlnet_ca.sel_model
Loading from saved_model/old_sqlnet_ca.cond_model
/home/team9/17IT208/SQLNet/sqlnet/model/modules/aggregator_predict.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  att = self.softmax(att_val)
/home/team9/17IT208/SQLNet/sqlnet/model/modules/selection_predict.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  att = self.softmax(att_val.view((-1, max_x_len))).view(
/home/team9/17IT208/SQLNet/sqlnet/model/modules/sqlnet_condition_predict.py:123: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  num_col_att = self.softmax(num_col_att_val)
/home/team9/17IT208/SQLNet/sqlnet/model/modules/sqlnet_condition_predict.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  num_att = self.softmax(num_att_val)
/home/team9/17IT208/SQLNet/sqlnet/model/modules/sqlnet_condition_predict.py:156: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  (-1, max_x_len))).view(B, -1, max_x_len)
/home/team9/17IT208/SQLNet/sqlnet/model/modules/sqlnet_condition_predict.py:202: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  op_att = self.softmax(op_att_val.view(B*4, -1)).view(B, 4, -1)
Init dev acc_qm: 0.536159600998
  breakdown on (agg, sel, where): [0.89834936 0.9035744  0.63484147]
Epoch 1 @ 2019-11-10 01:16:31.056471
 Loss = 0.8307988403027721
 Train acc_qm: 0.695998580428
   breakdown result: [0.91998935 0.97657706 0.77008251]
 Dev acc_qm: 0.542928393302
   breakdown result: [0.89894312 0.90571191 0.64149151]
 Best val acc = (0.8989431183944899, 0.9057119106994419, 0.6414915093219332), on epoch (1, 1, 1) individually
Epoch 2 @ 2019-11-10 01:29:44.114342
 Loss = 0.7116812571511697
 Train acc_qm: 0.735657883063
   breakdown result: [0.93056517 0.98097773 0.80220034]
 Dev acc_qm: 0.558603491272
   breakdown result: [0.89336183 0.91295571 0.65966037]
 Best val acc = (0.8989431183944899, 0.9129557059731623, 0.6596603728773305), on epoch (1, 2, 2) individually
Epoch 3 @ 2019-11-10 01:42:58.384410
 Loss = 0.6052366568936228
 Train acc_qm: 0.77095200071
   breakdown result: [0.95574483 0.98555585 0.81605891]
 Dev acc_qm: 0.548153425959
   breakdown result: [0.88136801 0.91331196 0.65799786]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6596603728773305), on epoch (1, 3, 2) individually
Epoch 4 @ 2019-11-10 01:56:10.558437
 Loss = 0.5208648914400703
 Train acc_qm: 0.798030343359
   breakdown result: [0.96972762 0.9878804  0.83163872]
 Dev acc_qm: 0.548153425959
   breakdown result: [0.87709298 0.91164945 0.66310414]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6631041444009025), on epoch (1, 3, 4) individually
Epoch 5 @ 2019-11-10 02:09:26.993148
 Loss = 0.44966240246974315
 Train acc_qm: 0.818312483364
   breakdown result: [0.97707391 0.98937095 0.84537308]
 Dev acc_qm: 0.545659660373
   breakdown result: [0.87792424 0.91069944 0.66025413]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6631041444009025), on epoch (1, 3, 4) individually
Epoch 6 @ 2019-11-10 02:22:42.626451
 Loss = 0.4002379724850577
 Train acc_qm: 0.837476710141
   breakdown result: [0.98387011 0.9905421  0.85846864]
 Dev acc_qm: 0.548390927443
   breakdown result: [0.86604916 0.90856193 0.67129795]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.67129794561216), on epoch (1, 3, 6) individually
Epoch 7 @ 2019-11-10 02:35:26.936400
 Loss = 0.36508769028549154
 Train acc_qm: 0.847644397125
   breakdown result: [0.98644308 0.99174874 0.86544229]
 Dev acc_qm: 0.551359695998
   breakdown result: [0.86925543 0.91212445 0.67046669]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.67129794561216), on epoch (1, 3, 6) individually
Epoch 8 @ 2019-11-10 02:46:27.338784
 Loss = 0.3333692626049457
 Train acc_qm: 0.864750243989
   breakdown result: [0.98997427 0.99309733 0.87825393]
 Dev acc_qm: 0.555159719748
   breakdown result: [0.86889918 0.9112932  0.67438546]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6743854649091556), on epoch (1, 3, 8) individually
Epoch 9 @ 2019-11-10 02:57:20.864883
 Loss = 0.3000954204139878
 Train acc_qm: 0.873249933458
   breakdown result: [0.99183746 0.99354095 0.88490817]
 Dev acc_qm: 0.55527847049
   breakdown result: [0.86569291 0.90963069 0.68115426]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6811542572141076), on epoch (1, 3, 9) individually
Epoch 10 @ 2019-11-10 03:08:17.466974
 Loss = 0.28508130107519525
 Train acc_qm: 0.883453109751
   breakdown result: [0.99359418 0.99391358 0.89379824]
 Dev acc_qm: 0.558722242014
   breakdown result: [0.86699917 0.90915568 0.68257927]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6825792661204132), on epoch (1, 3, 10) individually
Epoch 11 @ 2019-11-10 03:20:13.143892
 Loss = 0.2682196864299856
 Train acc_qm: 0.887694082158
   breakdown result: [0.9938426  0.9945169  0.89711649]
 Dev acc_qm: 0.558009737561
   breakdown result: [0.8645054  0.91141195 0.67925425]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6825792661204132), on epoch (1, 3, 10) individually
Epoch 12 @ 2019-11-10 03:33:18.550676
 Loss = 0.25394236588528857
 Train acc_qm: 0.893709520007
   breakdown result: [0.99488954 0.99504924 0.9019253 ]
 Dev acc_qm: 0.555634722717
   breakdown result: [0.86236789 0.91295571 0.679848  ]
 Best val acc = (0.8989431183944899, 0.9133119581997388, 0.6825792661204132), on epoch (1, 3, 10) individually
Epoch 13 @ 2019-11-10 03:46:29.168449
 Loss = 0.23884746540767807
 Train acc_qm: 0.903167420814
   breakdown result: [0.99501375 0.99545737 0.9108686 ]
 Dev acc_qm: 0.562997268733
   breakdown result: [0.86901793 0.91485572 0.68305427]
 Best val acc = (0.8989431183944899, 0.9148557178482366, 0.6830542690891818), on epoch (1, 13, 13) individually
Epoch 14 @ 2019-11-10 03:59:33.854504
 Loss = 0.2274213481241317
 Train acc_qm: 0.910336261201
   breakdown result: [0.99710762 0.9955106  0.91629847]
 Dev acc_qm: 0.565372283577
   breakdown result: [0.86011163 0.91319321 0.69065432]
 Best val acc = (0.8989431183944899, 0.9148557178482366, 0.6906543165894787), on epoch (1, 13, 14) individually
Epoch 15 @ 2019-11-10 04:12:47.936151
 Loss = 0.21552493936653178
 Train acc_qm: 0.909590985716
   breakdown result: [0.99730281 0.99629137 0.91480791]
 Dev acc_qm: 0.56394727467
   breakdown result: [0.86593041 0.9127182  0.68590429]
 Best val acc = (0.8989431183944899, 0.9148557178482366, 0.6906543165894787), on epoch (1, 13, 14) individually
Epoch 16 @ 2019-11-10 04:25:59.668208
 Loss = 0.20536104174382558
 Train acc_qm: 0.9197586727
   breakdown result: [0.99728507 0.99600745 0.92527726]
 Dev acc_qm: 0.558959743498
   breakdown result: [0.8565491  0.91069944 0.68899181]
 Best val acc = (0.8989431183944899, 0.9148557178482366, 0.6906543165894787), on epoch (1, 13, 14) individually
Epoch 17 @ 2019-11-10 04:39:11.903316
 